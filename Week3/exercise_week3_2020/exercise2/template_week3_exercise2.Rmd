---
title: "Censored observations"
subtitle: "Week3-ex2, solution"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
set.seed(1)
```

# Exercise instructions

This is the same exercise as 2.1 except that now the posterior inference is performed with MCMC using Stan. Hence, instead of the grid based approximation we use Monte Carlo approximation to do the same analyses as in exercise 2.1.

Suppose you have a $\text{Gamma}(\alpha=1,\beta=1)$ prior distribution on the parameter $\lambda$ which corresponds to the expected number of ship ice besetting events (=events where a ship gets stuck in ice) during 1000 nautical miles in ice infested waters. The number of besetting events, $y$ per distance $d$ (nm) is modeled with a Poisson distribution $\text{Poisson}(\lambda \times d)$.
The hyper-parameter $\alpha$ is the shape and $\beta$ is the inverse scale parameter. You are told that during winters 2013-2017 category A ice breakers traveled in total 6560 nautical miles in the Kara Sea (a sea area in the Arctic Sea). Within this distance they experienced in total more than 2 but less than 7 ice besetting events. 


\begin{itemize}
\item[1)] Implement the model with Stan and sample from the posterior of $\lambda$.
\begin{itemize}
\item[a)] Check for convergence visually and by calculating the PSRF statistics.
\item[b)] Calculate the autocorrelation of the samples.
\end{itemize}
\item[2)] Using the samples of $\lambda$ 
\begin{itemize}
\item[a)] draw the posterior density function of $\lambda$ and 
\item[b)] calculate the posterior probability that $\lambda<1$ and the 5\% and the 95\% quantiles.
\item[c)] calculate the posterior mean and variance of $\lambda$.
\end{itemize}
\item[3)] Draw samples from the posterior predictive distribution for new $\tilde{y}$ for a ship traveling 1500 nm distance and 
\begin{itemize}
\item[a)] draw histogram of samples from the posterior predictive distribution for $\tilde{y}$
\item[b)] Calculate the posterior predictive mean and variance of $\tilde{y}$
\end{itemize}
\end{itemize}


# Model answer

## 1-3)

The posterior probability density function in case of censored observation is 

$p(\lambda|2<y<7,d=6.56) \propto \left(\text{Poisson}(3|\lambda\times d)+\text{Poisson}(4|\lambda\times d)+\text{Poisson}(5|\lambda\times d)+\text{Poisson}(6|\lambda\times d)\right)\text{Gamma}(\lambda|1,1)$

When using Stan, we need to first load the needed libraries into R and define a Stan model

```{r}
library(ggplot2)
library(StanHeaders)
library(rstan)
library(coda)
set.seed(123)

options(mc.cores = parallel::detectCores())
rstan_options(auto_write = TRUE)
```

Please note that my model is written a little bit different from the usual ones. I discussed it with the TA and he validated it. In my case, y is hard coded. Instead of defining a vector 3<y<6 I treated each value as a variable alone. The function log_sum_exp can't take more than 2 arguments. That's why I sum up every two poisson_lpmf together and then sum their outputs.

```{r echo = T, results = 'hide'}
censored_observations_model="
  data{
    real<lower=0> d;
    int<lower=3, upper=3> y3; //
    int<lower=4, upper=4> y4; //
    int<lower=5, upper=5> y5; //
    int<lower=6, upper=6> y6; //
  }

  parameters{
    real<lower = 0> lambda;
  }

  model{
    lambda ~ gamma(1,1);
    target += log_sum_exp(log_sum_exp( poisson_lpmf(y3|lambda*d), poisson_lpmf(y4|lambda*d) ), \
    log_sum_exp(poisson_lpmf(y5|lambda*d), poisson_lpmf(y6|lambda*d))); 
  }
"
```


Now we can define the data list and run Markov chain with Stan

```{r}
d = 6.576
lambda = seq(0, 3, length=101) 

dataset <- list("y3" = 3, "y4" = 4, "y5" = 5, "y6" = 6, "d"= d, "lambda" = lambda)

#give initial values for all chains for parameter theta
init1 <- list (lambda = 100)
init2 <- list (lambda = 50)
init3 <- list (lambda = 200)

inits <- list(init1, init2, init3) 

# stan function does all of the work of fitting a Stan model and 
# returning the results as an instance of stanfit = post in our exercises.
post=stan(model_code=censored_observations_model,data=dataset,init=inits,
          warmup=500,iter=1500,chains=3,thin=1)
```

Next we can examine the posterior samples in various ways.

```{r}
# visual inspection 
plot(post, plotfun= "trace", pars ="lambda", inc_warmup = TRUE)

# Inspection with PSRF=Rhat
print(post,pars="lambda")

#Calculate the autocorrelation of the samples after removing burn-in.  Is autocorrelation
#a problem here?
lambdaSamp = as.matrix(post, pars ="lambda")

```
Visually, we can easily see that the 3 chains converge and oscillate around 1. 
By looking to the summary, we can see that Rhat = 1 which is < 1.05 therefore we have convergence.
```{r}
# Show pairs summary
pairs(post, pars=c('lambda', 'lp__'))
```

### 1b. Calculate the autocorrelation of the samples.
```{r}
stan_ac(post,"lambda",inc_warmup = FALSE, lags = 25)
```

##  2)

### 2.a
```{r}
#plot histogram of the posterior of lambda (approximation for density function)
plot(post, plotfun = "hist", pars = "lambda",bins=50, probs = c(0.025, 0.975))

# take samples of lambda into a vector
lambdaSamp=as.matrix(post, pars ="lambda")
```

### 2.b
```{r}
# Calculate the probability that lambda<1
PlambdaLessThan1 = sum( lambdaSamp < 1 ) / length(lambdaSamp)
paste("The proba of lambda being less than 1 is:",PlambdaLessThan1)

post_summary2 <- summary(post,pars="lambda",probs = c(0.05, 0.95))
post_summary2$summary
```
q 5% = 0.23
q 95% = 1.346

### 2.c   
```{r}

#calculate the posterior mean and variance
postMean = mean(lambdaSamp)
postVar = var(lambdaSamp)

paste("Posterior mean is equal to:",postMean)
paste("Posterior variance is equal to:",postVar)

```

## 3

Next we draw samples from the posterior predictive distribution for new $\tilde{y}$

The posterior predictive probability for $\tilde{y}$ is

$$p(\tilde{y}|2<y<7,d=6.56,\tilde{d}=1.5)=\sum_{i=1}^{100}\text{Poisson}(\tilde{y}\times 1.5|\lambda)p(\lambda|2<y<7,d=6.56)$$
### 3.a
```{r}
pred = rep(0,length(lambdaSamp))
for (i in 1:length(lambdaSamp)){
  pred[i] = rpois(1, lambdaSamp[i] * 1.5)
}
  hist(pred)
```

### 3.b
```{r}
predMeand = mean(pred)
varMean = var(pred)

paste("The mean is:" , predMeand)
paste("The var is:" , varMean)

```




# Grading

**Total 10 points:** 4 points for correclty doing step 1. 3 points for correctly doing step 2. 3 points for correctly doing step 3. 

